{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5625d460",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ae765-6ff5-4863-ad3d-426a39c52448",
   "metadata": {},
   "source": [
    "### Install huggingface datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f46c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efb493-c893-406c-9a34-400660781144",
   "metadata": {},
   "source": [
    "### Load dataset from huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6713547b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'privacy_mask', 'span_labels', 'bio_labels', 'tokenised_text'],\n",
       "        num_rows: 61918\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"TypicaAI/pii-masking-60k_fr\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69de1329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked_text': \"Cher [PREFIX_1] [LASTNAME_1], nous organisons un programme d'alphabétisation à [CITY_1] en collaboration avec [COMPANYNAME_1]. Contactez [EMAIL_1] pour plus de détails.\",\n",
       " 'unmasked_text': \"Cher Ms. Keebler, nous organisons un programme d'alphabétisation à West Shemar en collaboration avec Morissette - Russel. Contactez Hulda44@yahoo.com pour plus de détails.\",\n",
       " 'privacy_mask': \"{'[PREFIX_1]': 'Ms.', '[LASTNAME_1]': 'Keebler', '[CITY_1]': 'West Shemar', '[COMPANYNAME_1]': 'Morissette - Russel', '[EMAIL_1]': 'Hulda44@yahoo.com'}\",\n",
       " 'span_labels': \"[[0, 5, 'O'], [5, 8, 'PREFIX_1'], [8, 9, 'O'], [9, 16, 'LASTNAME_1'], [16, 67, 'O'], [67, 78, 'CITY_1'], [78, 101, 'O'], [101, 120, 'COMPANYNAME_1'], [120, 132, 'O'], [132, 149, 'EMAIL_1'], [149, 171, 'O']]\",\n",
       " 'bio_labels': ['O',\n",
       "  'B-PREFIX',\n",
       "  'I-PREFIX',\n",
       "  'B-LASTNAME',\n",
       "  'I-LASTNAME',\n",
       "  'I-LASTNAME',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-CITY',\n",
       "  'I-CITY',\n",
       "  'I-CITY',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-COMPANYNAME',\n",
       "  'I-COMPANYNAME',\n",
       "  'I-COMPANYNAME',\n",
       "  'I-COMPANYNAME',\n",
       "  'I-COMPANYNAME',\n",
       "  'I-COMPANYNAME',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'I-EMAIL',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'tokenised_text': ['cher',\n",
       "  'ms',\n",
       "  '.',\n",
       "  'ke',\n",
       "  '##eb',\n",
       "  '##ler',\n",
       "  ',',\n",
       "  'no',\n",
       "  '##us',\n",
       "  'organ',\n",
       "  '##ison',\n",
       "  '##s',\n",
       "  'un',\n",
       "  'programme',\n",
       "  'd',\n",
       "  \"'\",\n",
       "  'alphabet',\n",
       "  '##isation',\n",
       "  'a',\n",
       "  'west',\n",
       "  'she',\n",
       "  '##mar',\n",
       "  'en',\n",
       "  'collaboration',\n",
       "  'ave',\n",
       "  '##c',\n",
       "  'mori',\n",
       "  '##sse',\n",
       "  '##tte',\n",
       "  '-',\n",
       "  'russ',\n",
       "  '##el',\n",
       "  '.',\n",
       "  'contact',\n",
       "  '##ez',\n",
       "  'hu',\n",
       "  '##lda',\n",
       "  '##44',\n",
       "  '@',\n",
       "  'yahoo',\n",
       "  '.',\n",
       "  'com',\n",
       "  'pour',\n",
       "  'plus',\n",
       "  'de',\n",
       "  'details',\n",
       "  '.']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f7686",
   "metadata": {},
   "source": [
    "# Generate OCI Data Labeling dataset \n",
    "\n",
    "(jsonl metadata with text files to go to obj storage bucket) Import format = JSONL Consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26d0f6c-36bc-4deb-8fee-a19a693b9b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['masked_text', 'unmasked_text', 'privacy_mask', 'span_labels', 'bio_labels', 'tokenised_text'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = dataset[\"train\"].select(range(0,3000))\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b6fe51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset book_oci_nlp_labeling_ds_060324_3000 was created successfully in the bucket book_oci_nlp_labeling_bucket\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# Initialize OCI Object Storage Client with notebook session's resource principal\n",
    "signer = oci.auth.signers.get_resource_principals_signer()\n",
    "object_storage_client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)\n",
    "\n",
    "# Initialize Object Storage bucket infos\n",
    "namespace = object_storage_client.get_namespace().data #\"yz2wwgkgt8eh\"\n",
    "bucket_name = \"book_oci_nlp_labeling_bucket\" #\"ner_ar_iob_bucket\"\n",
    "\n",
    "# Base folder in the bucket\n",
    "base_folder = \"taln_pii_cs_ds_060324_3000/\"\n",
    "\n",
    "# Function to strip \"_1\" from labels\n",
    "def strip_label(label):\n",
    "    return label.rsplit(\"_\", 1)[0] if \"_\" in label else label\n",
    "\n",
    "labels_set = set()\n",
    "annotations_list = []\n",
    "\n",
    "for idx, item in enumerate(small_dataset):\n",
    "    text = item['unmasked_text']\n",
    "    span_labels = eval(item['span_labels'])  # Convert string to list if necessary\n",
    "\n",
    "    # exclude labels not relevant to our use-case\n",
    "    relevant_labels = {'EMAIL_1', 'CREDITCARDNUMBER_1', 'MIDDLENAME_1', 'FIRSTNAME_1', 'LASTNAME_1', 'AGE_1',\n",
    "                     'PHONENUMBER_1', 'STREET_1', 'ZIPCODE_1'}\n",
    "    \n",
    "    # Prepare annotations for this row\n",
    "    entities = []\n",
    "    for start, end, label in span_labels:\n",
    "        if label in relevant_labels:\n",
    "            label = strip_label(label)\n",
    "            labels_set.add(label)  # Add to the set of unique labels\n",
    "            entities.append({\n",
    "                \"entityType\": \"TEXTSELECTION\",\n",
    "                \"labels\": [{\"label_name\": label}],\n",
    "                \"textSpan\": {\"offset\": start, \"length\": end - start}\n",
    "            })\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "\n",
    "        file_name = f\"taln_pii_case_study_{idx}.txt\"\n",
    "\n",
    "        annotations_list.append({\n",
    "            \"sourceDetails\": {\"path\": file_name},\n",
    "            \"annotations\": [{\"entities\": entities}]\n",
    "        })\n",
    "\n",
    "        # Use delete=False because we need the file path for uploading\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.txt') as tmpfile:\n",
    "            tmpfile.write(text)\n",
    "            temp_file_path = tmpfile.name  # Store the temp file path to delete later\n",
    "\n",
    "        # Upload the text to OCI bucket\n",
    "        record_filename = f\"{base_folder}{file_name}\"\n",
    "        #record_body = io.BytesIO(text.encode('utf-8'))\n",
    "\n",
    "        with open(temp_file_path, 'rb') as f:\n",
    "            object_storage_client.put_object(namespace, \n",
    "                                             bucket_name, \n",
    "                                             record_filename, \n",
    "                                             f,\n",
    "                                             content_type='text/plain')\n",
    "        #print(f'Uploaded {temp_file_path} file to object {bucket_name}/{record_filename}')\n",
    "\n",
    "        # Delete the tempfile now that it's been uploaded\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the dataset details JSON\n",
    "dataset_details = {\n",
    "    \"displayName\": \"book_oci_nlp_labeling_ds_060324_3000\",\n",
    "    \"description\": \"book_oci_nlp_labeling_ds\",\n",
    "    \"labelsSet\": [{\"name\": label} for label in labels_set],\n",
    "    \"annotationFormat\": \"ENTITY_EXTRACTION\",\n",
    "    \"datasetFormatDetails\": {\"formatType\": \"TEXT\"}\n",
    "}\n",
    "\n",
    "# Metadata and annotations as JSONL string\n",
    "jsonl_data = json.dumps(dataset_details) + '\\n' + '\\n'.join(json.dumps(annotation) for annotation in annotations_list)\n",
    "\n",
    "# Use tempfile to create a temporary file for JSONL content\n",
    "with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.jsonl') as tmpfile:\n",
    "# Write the metadata and annotations to a JSONL file\n",
    "#with open('taln_pii_case_study_ds_metadata.jsonl', 'w') as f:\n",
    "    json.dump(dataset_details, tmpfile)\n",
    "    tmpfile.write('\\n')\n",
    "    for annotation in annotations_list:\n",
    "        json.dump(annotation, tmpfile)\n",
    "        tmpfile.write('\\n')\n",
    "    #tmpfile.write(jsonl_data)\n",
    "    tmpfile_path = tmpfile.name  # Save the path for uploading\n",
    "        \n",
    "# Upload the JSONL file to OCI bucket\n",
    "with open(tmpfile_path, 'rb') as f:\n",
    "    objStoreResp = object_storage_client.put_object(namespace, \n",
    "                                                    bucket_name, \n",
    "                                                    f\"{base_folder}dataset_metadata.jsonl\", \n",
    "                                                    f,\n",
    "                                                    content_type='application/json'  # Set the Content-Type for the object\n",
    "                                                   )\n",
    "\n",
    "print(f'The Dataset {dataset_details[\"displayName\"]} was created successfully in the bucket {bucket_name}')\n",
    "\n",
    "# Optionally delete the temporary file\n",
    "os.remove(tmpfile_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch21_p39_gpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch21_p39_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
